qgraph(cor(big5),minimum=0.25)
install.packages("RCurl")
save.image("~/.RData")
data(iris)
iris2<-subset(iris, Species!="setosa")
iris2$Species<-factor(iris2$Species)
lout<-glm(Species~., family="binomial", data=iris2)
summary(lout)
plot(ir2)
source('D:/Workspace/R_Workspace/practice0511.R', echo=TRUE)
library(neuralnet)
library(nnet) # using class.ind
data(iris)
species.ind <- class.ind(iris$Species)
install.packages("neuralnet")
library(neuralnet)
library(nnet) # using class.ind
data(iris)
species.ind <- class.ind(iris$Species)
iris <- cbind(iris, species.ind)
samp <- c(sample(1:50,25), sample(51:100,25),sample(101:150,25))
iris.tr<-iris[samp,]
iris.te<-iris[-samp,]
ir1 <- neuralnet(setosa+versicolor+virginica~Sepal.Length + Sepal.Width + Petal.Length +
Petal.Width, hidden=2, data=iris.tr)
ir2 <- neuralnet(setosa+versicolor+virginica~Sepal.Length + Sepal.Width + Petal.Length +
Petal.Width, hidden=c(3,3), data=iris.tr)
plot(ir1)
plot(ir2)
setwd("D:/Workspace/R_Workspace/Data/chapter 7")
##### Chapter 7: Neural Networks and Support Vector Machines -------------------
##### Part 1: Neural Networks -------------------
## Example: Modeling the Strength of Concrete  ----
## Step 2: Exploring and preparing the data ----
# read in data and examine structure
setwd("D:/Workspace/R_Workspace/Data/chapter 7")
concrete <- read.csv("concrete.csv")
str(concrete)
# custom normalization function
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# apply normalization to entire data frame
concrete_norm <- as.data.frame(lapply(concrete, normalize))
# confirm that the range is now between zero and one
summary(concrete_norm$strength)
# compared to the original minimum and maximum
summary(concrete$strength)
# create training and test data
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
## Step 3: Training a model on the data ----
# train the neuralnet model
library(neuralnet)
# simple ANN with only a single hidden neuron
concrete_model <- neuralnet(formula = strength ~ cement + slag +
ash + water + superplastic +
coarseagg + fineagg + age,
data = concrete_train)
# visualize the network topology
plot(concrete_model)
## Step 4: Evaluating model performance ----
# obtain model results
model_results <- compute(concrete_model, concrete_test[1:8])
# obtain predicted strength values
predicted_strength <- model_results$net.result
# examine the correlation between predicted and actual values
cor(predicted_strength, concrete_test$strength)
## Step 5: Improving model performance ----
# a more complex neural network topology with 5 hidden neurons
concrete_model2 <- neuralnet(strength ~ cement + slag +
ash + water + superplastic +
coarseagg + fineagg + age,
data = concrete_train, hidden = 5)
# plot the network
plot(concrete_model2)
# evaluate the results as we did before
model_results2 <- compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
##### Part 2: Support Vector Machines -------------------
## Example: Optical Character Recognition ----
## Step 2: Exploring and preparing the data ----
# read in data and examine structure
letters <- read.csv("letterdata.csv")
str(letters)
# divide into training and test data
letters_train <- letters[1:16000, ]
letters_test  <- letters[16001:20000, ]
## Step 3: Training a model on the data ----
# begin by training a simple linear SVM
library(kernlab)
letter_classifier <- ksvm(letter ~ ., data = letters_train,
kernel = "vanilladot")
# look at basic information about the model
letter_classifier
## Step 4: Evaluating model performance ----
# predictions on testing dataset
letter_predictions <- predict(letter_classifier, letters_test)
head(letter_predictions)
table(letter_predictions, letters_test$letter)
# look only at agreement vs. non-agreement
# construct a vector of TRUE/FALSE indicating correct/incorrect predictions
agreement <- letter_predictions == letters_test$letter
table(agreement)
prop.table(table(agreement))
## Step 5: Improving model performance ----
letter_classifier_rbf <- ksvm(letter ~ ., data = letters_train, kernel = "rbfdot")
letter_predictions_rbf <- predict(letter_classifier_rbf, letters_test)
agreement_rbf <- letter_predictions_rbf == letters_test$letter
table(agreement_rbf)
prop.table(table(agreement_rbf))
letters <- read.csv("letterdata.csv")
str(letters)
# divide into training and test data
letters_train <- letters[1:16000, ]
letters_test  <- letters[16001:20000, ]
## Step 3: Training a model on the data ----
# begin by training a simple linear SVM
library(kernlab)
letter_classifier <- ksvm(letter ~ ., data = letters_train,
kernel = "vanilladot")
# look at basic information about the model
letter_classifier
## Step 4: Evaluating model performance ----
# predictions on testing dataset
letter_predictions <- predict(letter_classifier, letters_test)
head(letter_predictions)
table(letter_predictions, letters_test$letter)
# look only at agreement vs. non-agreement
# construct a vector of TRUE/FALSE indicating correct/incorrect predictions
agreement <- letter_predictions == letters_test$letter
table(agreement)
prop.table(table(agreement))
## Step 5: Improving model performance ----
letter_classifier_rbf <- ksvm(letter ~ ., data = letters_train, kernel = "rbfdot")
letter_predictions_rbf <- predict(letter_classifier_rbf, letters_test)
agreement_rbf <- letter_predictions_rbf == letters_test$letter
table(agreement_rbf)
prop.table(table(agreement_rbf))
setwd("D:/Workspace/R_Workspace/Mnist")
load_mnist <- function() {
load_image_file <- function(filename) {
ret = list()
f = file(filename,'rb')
readBin(f,'integer',n=1,size=4,endian='big')
ret$n = readBin(f,'integer',n=1,size=4,endian='big')
nrow = readBin(f,'integer',n=1,size=4,endian='big')
ncol = readBin(f,'integer',n=1,size=4,endian='big')
x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
close(f)
ret
}
load_label_file <- function(filename) {
f = file(filename,'rb')
readBin(f,'integer',n=1,size=4,endian='big')
n = readBin(f,'integer',n=1,size=4,endian='big')
y = readBin(f,'integer',n=n,size=1,signed=F)
close(f)
y
}
train <<- load_image_file('mnist/train-images-idx3-ubyte')
test <<- load_image_file('mnist/t10k-images-idx3-ubyte')
train$y <<- load_label_file('mnist/train-labels-idx1-ubyte')
test$y <<- load_label_file('mnist/t10k-labels-idx1-ubyte')
}
show_digit <- function(arr784, col=gray(12:1/12), ...) {
image(matrix(arr784, nrow=28)[,28:1], col=col, ...)
}
load_mnist("t10k-images.idx3-ubyte")
filename = "t10k-images.idx3-ubyte"
load_mnist()
load_mnist <- function() {
filename = "t10k-images.idx3-ubyte"
load_image_file <- function(filename) {
ret = list()
f = file(filename,'rb')
readBin(f,'integer',n=1,size=4,endian='big')
ret$n = readBin(f,'integer',n=1,size=4,endian='big')
nrow = readBin(f,'integer',n=1,size=4,endian='big')
ncol = readBin(f,'integer',n=1,size=4,endian='big')
x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
close(f)
ret
}
load_label_file <- function(filename) {
f = file(filename,'rb')
readBin(f,'integer',n=1,size=4,endian='big')
n = readBin(f,'integer',n=1,size=4,endian='big')
y = readBin(f,'integer',n=n,size=1,signed=F)
close(f)
y
}
train <<- load_image_file('mnist/train-images-idx3-ubyte')
test <<- load_image_file('mnist/t10k-images-idx3-ubyte')
train$y <<- load_label_file('mnist/train-labels-idx1-ubyte')
test$y <<- load_label_file('mnist/t10k-labels-idx1-ubyte')
}
load_mnist()
load_mnist <- function() {
filename = "mnist_train.csv"
load_image_file <- function(filename) {
ret = list()
f = file(filename,'rb')
readBin(f,'integer',n=1,size=4,endian='big')
ret$n = readBin(f,'integer',n=1,size=4,endian='big')
nrow = readBin(f,'integer',n=1,size=4,endian='big')
ncol = readBin(f,'integer',n=1,size=4,endian='big')
x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
close(f)
ret
}
load_label_file <- function(filename) {
f = file(filename,'rb')
readBin(f,'integer',n=1,size=4,endian='big')
n = readBin(f,'integer',n=1,size=4,endian='big')
y = readBin(f,'integer',n=n,size=1,signed=F)
close(f)
y
}
train <<- load_image_file('mnist/train-images-idx3-ubyte')
test <<- load_image_file('mnist/t10k-images-idx3-ubyte')
train$y <<- load_label_file('mnist/train-labels-idx1-ubyte')
test$y <<- load_label_file('mnist/t10k-labels-idx1-ubyte')
}
load_mnist()
readMNIST("D:/Workspace/R_Workspace/Mnist")
setwd("D:/Workspace/R_Workspace/Mnist")
R.utils::gunzip("train-images-idx3-ubyte.gz")
install.packages("R.utils")
show_digit = function(arr784, col = gray(12:1 / 12), ...) {
image(matrix(as.matrix(arr784[-785]), nrow = 28)[, 28:1], col = col, ...)
}
load_image_file = function(filename) {
ret = list()
f = file(filename, 'rb')
readBin(f, 'integer', n = 1, size = 4, endian = 'big')
n    = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
nrow = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
ncol = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
x = readBin(f, 'integer', n = n * nrow * ncol, size = 1, signed = FALSE)
close(f)
data.frame(matrix(x, ncol = nrow * ncol, byrow = TRUE))
}
# load label files
load_label_file = function(filename) {
f = file(filename, 'rb')
readBin(f, 'integer', n = 1, size = 4, endian = 'big')
n = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
y = readBin(f, 'integer', n = n, size = 1, signed = FALSE)
close(f)
y
}
train = load_image_file("train-images-idx3-ubyte")
test  = load_image_file("t10k-images-idx3-ubyte")
# load labels
train$y = as.factor(load_label_file("train-labels-idx1-ubyte"))
test$y  = as.factor(load_label_file("t10k-lab
# load images
train = load_image_file("train-images-idx3-ubyte")
# load images
train = load_image_file("train-images-idx3-ubyte")
# load images
train = load_image_file("train-images.idx3-ubyte")
test  = load_image_file("t10k-images.idx3-ubyte")
# load labels
train$y = as.factor(load_label_file("train-labels.idx1-ubyte"))
test$y  = as.factor(load_label_file("t10k-labels.idx1-ubyte"))
# view test image
show_digit(train[10000, ])
# testing classification on subset of training data
fit = randomForest::randomForest(y ~ ., data = train[1:1000, ])
library(nnet)
help nnet
help
nnet
to.read= file("t10k-images.idx3-ubyte", "rb")
readBin(to.read, integer(), n=4, endian="big")
par(mfrow=c(5,5))
par(mar=c(0,0))
par(mar=c(0,0))
for(i in 1:25){m = matrix(readBin(to.read,integer(), size=1, n=28*28, endian="big"),28,28);image(m[,28:1])}
test_pred = predict(fit, test)
for(i in 1:25){
m = matrix(readBin(to.read,integer(), size=1, n=28*28,
endian="big"),28,28);image(m[,28:1])}
par(mfrow=c(5,5))
par
par?
.
?par
for(i in 1:25){
m = matrix(readBin(to.read,integer(), size=1, n=28*28,
endian="big"),28,28);image(m[,28:1])}
for(i in 1:25){
m = matrix(readBin(to.read,integer(), size=1, n=28*28,
endian="big"),28,28);image(m[,28:1])}
to.read= file("train-images.idx3-ubyte", "rb")
readBin(to.read, integer(), n=4, endian="big")
par(mfrow=c(5,5))
for(i in 1:25){
m = matrix(readBin(to.read,integer(), size=1, n=28*28,
endian="big"),28,28);image(m[,28:1])}
setwd("D:\Workspace\R_Workspace\Data\chapter 7")
letterdata <- read.csv("letterdata.csv")
setwd("D:/Workspace/R_Workspace/Data/chapter 7")
letterdata <- read.csv("letterdata.csv")
str(letterdata)
# custom normalization function
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# apply normalization to entire data frame
letterdata_norm <- as.data.frame(lapply(letterdata, normalize))
str(letterdata)
# apply normalization to entire data frame
letter_input_norm <- as.data.frame(lapply(letterdata[2:17], normalize))
# confirm that the range is now between zero and one
summary(letter_input_norm$xbox)
# compared to the original minimum and maximum
summary(letterdata$xbox)
setwd("D:/Workspace/R_Workspace/Data/chapter 7")
letterdata <- read.csv("letterdata.csv")
# custom normalization function
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# apply normalization to entire data frame
letter_features_norm <- as.data.frame(lapply(letterdata[2:17], normalize))
# create training and test data
letter_features_train <- letter_features_norm[1:15000, ]
letter_features_test <- letter_features_norm[15001:20000, ]
?nnet
?neuralnet
library(nnet)
?neuralnet
## Step 3: Training a model on the data ----
# train the neuralnet model
library(nnet)
class.ind(letterdata[1])
letterdata[1]
letter_label_train <- letterdata[1][1:15000, ]
letter_label_test <- letterdata[1][15001:20000, ]
cat("HI")
str(letter_features_norm)
?nnet
nnet(letter_features_train, letter_label_train, weights, size, Wts, mask,
linout = FALSE, entropy = FALSE, softmax = FALSE,
censored = FALSE, skip = FALSE, rang = 0.7, decay = 0,
maxit = 100, Hess = FALSE, trace = TRUE, MaxNWts = 1000,
abstol = 1.0e-4, reltol = 1.0e-8, ...)
nnet(letter_features_train, letter_label_train, weights, size, Wts, mask,
linout = FALSE, entropy = FALSE, softmax = FALSE,
censored = FALSE, skip = FALSE, rang = 0.7, decay = 0,
maxit = 100, Hess = FALSE, trace = TRUE, MaxNWts = 1000,
abstol = 1.0e-4, reltol = 1.0e-8)
fit
ANN <- nnet(letter_label_train ~ . data=letter_features_train , decay=5e-4, maxit=200)
ANN <- nnet(letter_label_train ~ . data=letter_features_train, decay=5e-4, maxit=200)
ANN <- nnet(letter_label_train ~ ., data=letter_features_train, decay=5e-4, maxit=200)
ANN <- nnet(letter_label_train ~ ., data=letter_features_train, size =  decay=5e-4, maxit=200)
ANN <- nnet(letter_label_train ~ ., data=letter_features_train, size,  decay=5e-4, maxit=200)
ANN <- nnet(letter_label_train ~ ., data=letter_features_train, size = 2,  decay=5e-4, maxit=200)
plot(ANN)
pred <- predict(ANN, letter_features_test, type="class")
weighted
weighted.acc
?weighted.acc
pred_train <- predict(ANN, letter_features_train, type="class")
pred_test <- predict(ANN, letter_features_test, type="class")
train_accuracy <- weighted.acc(pred_train, letter_label_train) # weighted training accuracy
?weighted
??weighted
??weighted.acc
weighted.acc <- function(predictions, actual)
{
freqs <- as.data.frame(table(actual))
tmp <- t(mapply(function (p, a) { c(a, p==a) }, predictions, actual, USE.NAMES=FALSE)) # map over both together
tab <- as.data.frame(table(tmp[,1], tmp[,2])[,2]) # gives rows of [F,T] counts, where each row is a state
acc.pc <- tab[,1]/freqs[,2]
return(sum(acc.pc)/length(acc.pc))
}
train_accuracy <- weighted.acc(pred_train, letter_label_train) # weighted training accuracy
test_accuracy <- weighted.acc(pred_test, letter_label_test) # weighted test accuracy
train_accuracy <- weighted.acc(pred_train, letter_label_train) # weighted training accuracy
ANN <- nnet(letter_label_train ~ ., data=letter_features_train, size = 2,  decay=5e-4 )
ANN <- nnet(letter_label_train ~ ., data=letter_features_train, size = 5,  decay=5e-4, maxit=200)
?nnet
str(ANN)
t.start<-Sys.time()
t.end <-Sys.time()
t.end-t.start
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train, size = 50,  decay=5e-4, maxit=300)
memory.limit(40000)
memory.limit(20000)
memory.limit(10000)
memory.limit(50000)
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train, size = 10,  decay=5e-4, maxit=300)
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train, size = 10,  decay=1e-2, maxit=300)
pred_train <- predict(nnet_model, letter_features_train, type="class")
pred_test <- predict(nnet_model, letter_features_test, type="class")
table(letter_label_train, pred_train)
sum(diag(train_table)/sum(train_table))
train_table <- table(letter_label_train, pred_train)
test_table <- table(letter_label_test, pred_test)
sum(diag(train_table)/sum(train_table))
sum(diag(test_table)/sum(test_table))
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train, size = 20,  decay=1e-1, maxit=300)
setwd("D:/Workspace/R_Workspace/Data/chapter 7")
letterdata <- read.csv("letterdata.csv")
#str(letterdata)
# custom normalization function
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# apply normalization to entire data frame
letter_features_norm <- as.data.frame(lapply(letterdata[2:17], normalize))
# confirm that the range is now between zero and one
#summary(letter_features_norm$xbox)
# compared to the original minimum and maximum
#summary(letterdata$xbox)
# create training and test data
letter_features_train <- letter_features_norm[1:15000, ]
letter_label_train <- letterdata[1][1:15000, ]
letter_features_test <- letter_features_norm[15001:20000, ]
letter_label_test <- letterdata[1][15001:20000, ]
## Step 3: Training a model on the data ----
# train the neuralnet model
library(nnet)
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train, size = 20,  decay=1e-1, maxit=300)
pred_train <- predict(nnet_model, letter_features_train, type="class")
train_table <- table(letter_label_train, pred_train)
pred_test <- predict(nnet_model, letter_features_test, type="class")
test_table <- table(letter_label_test, pred_test)
sum(diag(train_table)/sum(train_table))
sum(diag(test_table)/sum(test_table))
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train, size = 50,  decay=1e-1, maxit=300)
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train, size = 40,  decay=1e-1, maxit=500)
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train, size = 35,  decay=1e-1, maxit=500)
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train, size = 30,  decay=1e-1, maxit=500)
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train, size = 25,  decay=1e-1, maxit=500)
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train, size = 24,  decay=1e-1, maxit=500)
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train, size = 23,  decay=1e-1, maxit=500)
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train, size = 22,  decay=1e-1, maxit=500)
plot(nnet_model)
type(nnet_model)
pred_train <- predict(nnet_model, letter_features_train, type="class")
pred_test <- predict(nnet_model, letter_features_test, type="class")
train_table <- table(letter_label_train, pred_train)
test_table <- table(letter_label_test, pred_test)
sum(diag(train_table)/sum(train_table))
sum(diag(test_table)/sum(test_table))
setwd("D:/Workspace/R_Workspace/Data/chapter 7")
letterdata <- read.csv("letterdata.csv")
str(letterdata)
# confirm that the range is now between zero and one
summary(letter_features_norm$xbox)
# compared to the original minimum and maximum
summary(letterdata$xbox)
# compared to the original minimum and maximum
summary(letterdata)
# confirm that the range is now between zero and one
summary(letter_features_norm$xbox)
library(nnet)
t.start <- Sys.time()
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train, size = 22,  decay=5e-2, maxit=1000)
t.end <- Sys.time()
t.end-t.start
pred_train <- predict(nnet_model, letter_features_train, type="class")
pred_test <- predict(nnet_model, letter_features_test, type="class")
train_table <- table(letter_label_train, pred_train)
test_table <- table(letter_label_test, pred_test)
sum(diag(train_table)/sum(train_table))
sum(diag(test_table)/sum(test_table))
library(nnet)
t.start <- Sys.time()
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train,
size = 22, decay=5e-2, maxit=3000)
t.start <- Sys.time()
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train,
size = 22, decay=5e-2, maxit=1000)
t.start <- Sys.time()
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train,
size = 22, decay=5e-2, maxit=100)
t.end <- Sys.time()
t.end-t.start
t.start <- Sys.time()
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train,
size = 22, decay=5e-4, maxit=100)
t.end <- Sys.time()
t.end-t.start
library(nnet)
t.start <- Sys.time()
nnet_model <- nnet(letter_label_train ~ ., data=letter_features_train,
size = 22, decay=5e-4, maxit=5000)
t.end <- Sys.time()
t.end-t.start
pred_train <- predict(nnet_model, letter_features_train, type="class")
train_table <- table(letter_label_train, pred_train)
sum(diag(train_table)/sum(train_table))
pred_test <- predict(nnet_model, letter_features_test, type="class")
test_table <- table(letter_label_test, pred_test)
sum(diag(test_table)/sum(test_table))
table(letter_label_train, pred_train)
table(letter_label_train, pred_train)
pred_test <- predict(nnet_model, letter_features_test, type="class")
test_table <- table(letter_label_test, pred_test)
sum(diag(test_table)/sum(test_table))
table(letter_label_test, pred_test)
