4 * x -> z
print(x)
print(z)
ls()
objects()
x <-1
objects()
setwd("D:/Workspace/R_Workspace")
print(x)
setwd("D:/Workspace/R_Workspace")
library(varhandle)
intstall package varhandle
intstall varhandle
source('D:/Workspace/R_Workspace/practice0316.R', echo=TRUE)
source('D:/Workspace/R_Workspace/practice0316.R', echo=TRUE)
install.packages("varhandle")
source('D:/Workspace/R_Workspace/practice0316.R', echo=TRUE)
source('D:/Workspace/R_Workspace/practice0316.R', echo=TRUE)
is.vector(temp)
sd(bcData$radius_mean)
mean(bcData$radius_mean)
median(bcData$radius_mean)
min(bcData$radius_mean)
which.min(bcData$radius_mean)
prop.table(table(bcData$diagnosis))
bcData$radius_mean[which.min(bcData$radius_mean)]
class(temp)
is.vector(temp)
temp2 <- bcData$diagnosis
class(temp2)
is.vector(temp2)
setwd("D:/Workspace/R_Workspace/Data/chapter 3")
setwd("D:/Workspace/R_Workspace/Data/chapter 3")
# import the CSV file
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
print(wbcd)
# examine the structure of the wbcd data frame
str(wbcd)
# drop the id feature
wbcd <- wbcd[-1]
# examine the structure of the wbcd data frame
str(wbcd)
# table of diagnosis
table(wbcd$diagnosis)
# recode diagnosis as a factor
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant"))
# table of diagnosis
table(wbcd$diagnosis)
# table or proportions with more informative labels
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
# summarize three numeric features
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
# test normalization function - result should be identical
normalize(c(1, 2, 3, 4, 5))
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# test normalization function - result should be identical
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
# normalize the wbcd data
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
# confirm that normalization worked
summary(wbcd_n$area_mean)
# confirm that normalization worked
summary(wbcd_n[c("radius_mean", "area_mean", "smoothness_mean")])
# create training and test data
wbcd_train <- wbcd_n[1:469, ] # 열에 아무것도 안주면 모든 열을 다 Select 하는
wbcd_test <- wbcd_n[470:569, ]
# load the "class" library
library(class)
display(wbcd_n)
print(wbcd_n)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, # ---------------------------------??
cl = wbcd_train_labels, k=1)#k=21)
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, # ---------------------------------??
cl = wbcd_train_labels, k=1)#k=21)
wbcd_test_pred
# load the "gmodels" library
library(gmodels)
# install.packages("gmodels")
install.packages("varhandle")
install.packages("varhandle")
install.packages("gmodels")
# load the "gmodels" library
library(gmodels)
# Create the cross tabulation of predicted vs. actual
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, # ---------------------------------??
prop.chisq=FALSE)
# use the scale() function to z-score standardize a data frame
wbcd_z <- as.data.frame(scale(wbcd[-1])) # ---------------------------------??
# confirm that the transformation was applied correctly
summary(wbcd_z$area_mean)
# create training and test datasets
wbcd_train <- wbcd_z[1:469, ]
wbcd_test <- wbcd_z[470:569, ]
# re-classify test cases
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k=21)
# Create the cross tabulation of predicted vs. actual
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
# try several different values of k
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=1)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=5)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=11)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=15)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=27)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
# create training and test datasets
samp <- sample(c(1:569), 100)# 랜덤하게 100개 뽑기
wbcd_train <- wbcd_z[-samp, ]
wbcd_test <- wbcd_z[samp, ]
wbcd_train_labels <- wbcd[-samp, 1]
wbcd_test_labels <- wbcd[samp, 1]
# re-classify test cases
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k=5)
# Create the cross tabulation of predicted vs. actual
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
# Create the cross tabulation of predicted vs. actual
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=TRUE)
data(iris)
view(iris)
view(iris)
range(bcData$radius_mean)
summary(bcData$radius_mean)
print(iris)
view(iris)
view()
view(data(iris))
View(iris)
var(bcData$radius_mean)
sd(bcData$radius_mean)
library(fBasics)
install.packages("fBasics")
str(iris)
str(iris)
iris
iris[-1]
table(iris)
table(iris)
round(prop.table(table(iris$Species)) * 100, digits = 1)
prop.table(table(iris$Species)
prop.table(table(iris$Species))
prop.table(table(iris$Species))
prop.table(iris$Species)
table(iris)
table(iris)
summary(iris)
iris_n <- as.data.frame(lapply(iris[1:4], normalize))
View(iris_n)
summary(iris_n)
iris_n <- as.data.frame(lapply(iris[1:5], normalize))
iris_n <- as.data.frame(lapply(iris[1:4], normalize))
summary(iris_n)
sample(c(1:50), 33)
cbind(1:2, 3:4)
rbind(1:2, 3:4)
a = 1:3; b = 5:8
a
b
a+b
rbind(a,b)
c(a,b)
iris_z <- as.data.frame(scale(iris[1:4]))
summary(iris_z)
samp <- c(sample(c(1:50), 33),sample(c(51:100), 33),sample(c(101:150), 34))
# 각 종류별로 균등하게 랜덤하게 100개 뽑기 (33개, 33개, 34개)
iris_train <- iris_n[-samp, ]
iris_test <- iris_n[samp, ]
iris_train_labels <- iris[-samp, 5]
iris_test_labels <- iris[samp, 5]
iris_test_pred <- knn(train = iris_train, test = iris_test,
cl = iris_train_labels, k=5)
CrossTable(x = iris_test_labels, y = iris_test_pred,
prop.chisq=FALSE)
samp <- c(sample(c(1:50), 33),sample(c(51:100), 33),sample(c(101:150), 34))
# 각 종류별로 균등하게 랜덤하게 100개 뽑기 (33개, 33개, 34개)
iris_train <- iris_n[-samp, ]
iris_test <- iris_n[samp, ]
iris_train_labels <- iris[-samp, 5]
iris_test_labels <- iris[samp, 5]
iris_test_pred <- knn(train = iris_train, test = iris_test,
cl = iris_train_labels, k=5)
CrossTable(x = iris_test_labels, y = iris_test_pred,
prop.chisq=FALSE)
samp <- c(sample(c(1:50), 33),sample(c(51:100), 34),sample(c(101:150), 33))
# 각 종류별로 균등하게 랜덤하게 100개 뽑기 (33개, 33개, 34개)
iris_train <- iris_n[-samp, ]
iris_test <- iris_n[samp, ]
iris_train_labels <- iris[-samp, 5]
iris_test_labels <- iris[samp, 5]
iris_test_pred <- knn(train = iris_train, test = iris_test,
cl = iris_train_labels, k=5)
CrossTable(x = iris_test_labels, y = iris_test_pred,
prop.chisq=FALSE)
cross_t = CrossTable(x = iris_test_labels, y = iris_test_pred,
prop.chisq=FALSE)
cross_t
cross_t$prop.tbl
cross_t$prop.tbl(1)
cross_t$prop.tbl(1,1)
cross_t$prop.tbl[1]
cross_t$prop.tbl[1:9]
seq(1,3)
seq(1,3,by=3)
seq(1,by=3,length=3)
cross_t$prop.tbl[1:9 - [1, 4, 9]]
cross_t$prop.tbl[1:9-[1,4,9]]
1:9
[1,4,9]
1,4,9
(1,4,9)
c(1,4,9)
1:9-c(1,4,9)
c(1,4,9)[1]
c(1,4,9)[0]
c(1,4,9)[1]
cross_t$prop.tbl[2,3,4,6,7,8]
cross_t$prop.tbl[2,3,4,6,7,8]
cross_t$prop.tbl[[2,3,4,6,7,8]]
cross_t$prop.tbl[1:9]
cross_t$prop.tbl[1:3,1:4]
cross_t$prop.tbl[1,2]
cross_t$prop.tbl[1,3]
cross_t$prop.tbl[1,4]
cross_t$prop.tbl[3,3]
cross_t$prop.tbl[1:9]
sum(cross_t$prop.tbl[1:9])
cross_t$prop.tbl[1&3]
cross_t$prop.tbl[1&2]
sum(cross_t$prop.tbl[1:9])-(cross_t$prop.tbl[1,1]+cross_t$prop.tbl[2,2]+cross_t$prop.tbl[3:3])
sum(cross_t$prop.tbl[1:9])-(cross_t$prop.tbl[1,1]+cross_t$prop.tbl[2,2]+cross_t$prop.tbl[3:3])
(cross_t$prop.tbl[1,1]+cross_t$prop.tbl[2,2]+cross_t$prop.tbl[3:3])
cross_t$prop.tbl[3:3]
sum(cross_t$prop.tbl[1:9])-(cross_t$prop.tbl[1,1]+cross_t$prop.tbl[2,2]+cross_t$prop.tbl[3,3])
a = 1
a
a+1
a+=1
a<-a+1
a
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R', echo=TRUE)
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R', echo=TRUE)
x<-1
x <- 1
x <- 2
x <- 2 ##
x
x ##
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source("2148_03.R")
source("2148_03.R")
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R', echo=TRUE)
print("hello")
print("hello","hello")
View("hello")
print("hello")print("hello")
print("hello");print("hello")
3%/%3
2%/%3
View(iris)
0:3
3/3
3%3
3%%3
2%%3%/%2
1%%3%/%2
0%%3%/%2
kl = 33 + 0%%3%/%2
kl
kl = 33 + 2%%3%/%2
kl
for(nc in 0:2)( # 종류별 train 개수를 변화시키기 위한 반복문
# nc : number change
setosa_tnum <- 33 + (nc%%3)%/%2
versicolor_tnum <- 33 + ((nc+1)%%3)%/%2
virginica_tnum <- 33 + ((nc+2)%%3)%/%2
log <- capture.output( # 불필요한 콘솔창 반복 출력으로 인한 시스템 저하를 방지하기 위함
for(kval in 1:30){
mispredict_sum <- 0
for(i in 1:N){ # 평균 오차율을 구하기 위한 반복 횟수
samp <- c(sample(c(1:50), 33),sample(c(51:100), 33),sample(c(101:150), 34))
# 각 종류별로 균등하게 랜덤하게 100개 뽑기 (33개, 33개, 34개)
iris_train <- iris_n[-samp, ]
iris_test <- iris_n[samp, ]
iris_train_labels <- iris[-samp, 5]
iris_test_labels <- iris[samp, 5]
iris_test_pred <- knn(train = iris_train, test = iris_test,
cl = iris_train_labels, k=kval)
ct <- CrossTable(x = iris_test_labels, y = iris_test_pred,
prop.chisq=FALSE)
mispredict_sum <- mispredict_sum +
sum(ct$prop.tbl[1:9])-(ct$prop.tbl[1,1]+ct$prop.tbl[2,2]+ct$prop.tbl[3,3])
}
mispredict_avg <- mispredict_sum/N
if (mispredict_avg < lowest_mispredict_avg){
lowest_mispredict_avg <- mispredict_avg
opt_k <- kval
}
}
)
a = 2
b = 3
print(a)
print(a,b)
print(b)
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
train_eachnum <- c(setosa_tnum,versicolor_tnum,virginica_tnum)
train_eachnum
train_eachnum <- as.data.frame(c(setosa_tnum,versicolor_tnum,virginica_tnum))
train_eachnum
train_eachnum <- cbind(c(setosa_tnum,versicolor_tnum,virginica_tnum))
train_eachnum
train_eachnum <- cbind(setosa_tnum,versicolor_tnum,virginica_tnum)
train_eachnum
colnames(train_eachnum) <- c("setosa","versicolor","virginica")
train_eachnum
rownames(train_eachnum) <- c("training number")
train_eachnum
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
sprintf("%d",N)
sprintf("뭐래니 %d",N)
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
source('D:/Workspace/R_Workspace/HW1_IRIS_0316.R')
summary(iris_z)
summary(iris_n)
str(iris_n)
